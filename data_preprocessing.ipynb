{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(x):\n",
    "    for s in ['#', '@', '_', '.', '?', '!', '*', '~', '%', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=>']:\n",
    "        x = x.replace(s, '')\n",
    "    x = x.replace('-', ' ')\n",
    "    return x\n",
    "\n",
    "# left only words that can be found in dictionary\n",
    "def text_remove_unknown_words(x, model):\n",
    "#     x = x.split(' ')\n",
    "    x_ = list()\n",
    "    for word in x:\n",
    "        try:\n",
    "            v = model[word]\n",
    "            x_.append(word)\n",
    "        except:\n",
    "            continue\n",
    "    return x_\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def n_punctuations(text):\n",
    "    count = 0\n",
    "    for s in ['!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~']:\n",
    "        if s in text:\n",
    "            c = text.count(s)\n",
    "            count+=c\n",
    "    return count\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"i'll\", \"i will\", text)\n",
    "    text = re.sub(r\"she'll\", \"she will\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"here's\", \"here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text) # This removes anything other than lower case letters(very imp)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    try:\n",
    "        pos_tagged_text = nltk.pos_tag(text.split())\n",
    "        return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, model,create_embedding = True, remove_punct = True, min_len = 3, remove_unknown_words = True, small_word = 3,\n",
    "                      replace_nan = True):\n",
    "    df['copy_text'] = df['text'].str.lower()\n",
    "    df['is_question'] = df['copy_text'].apply(lambda x: 1 if '?' in x else 0)\n",
    "    df['n_punctuation'] = df['copy_text'].apply(n_punctuations)\n",
    "    df['n_hashtag'] = df['copy_text'].apply(lambda x: x.count('#') if type(x) == str else 0)\n",
    "    # remove url and html\n",
    "    df['copy_text'] = df['copy_text'].apply(lemmatize_words)\n",
    "    df['copy_text'] = df['copy_text'].apply(clean_text)\n",
    "    df['copy_text'] = df['copy_text'].apply(remove_URL)\n",
    "    df['copy_text'] = df['copy_text'].apply(remove_html)\n",
    "    df['copy_text'] = df['copy_text'].apply(remove_emoji)\n",
    "    df['copy_text'] = df['copy_text'].apply(remove_punctuation)\n",
    "#     df['copy_text'] = df['copy_text'].apply(correct_spellings)\n",
    "#     print('5')\n",
    "    if remove_punct:\n",
    "        df['copy_text'] = df['copy_text'].apply(text_preprocessing)\n",
    "    df['copy_text'] = df['copy_text'].str.split(' ')\n",
    "    # remove small words\n",
    "    df['n_small_words'] = df['copy_text'].apply(lambda x: len([w for w in x if len(w) <= small_word]))\n",
    "    df['copy_text'] = df['copy_text'].apply(lambda x: [w for w in x if len(w) >=min_len])\n",
    "    # remove unknown words\n",
    "    if remove_unknown_words:\n",
    "        df['copy_text'] = df['copy_text'].apply(text_remove_unknown_words, args = (model,))\n",
    "    # create additional features\n",
    "    df['n_words'] = df['copy_text'].apply(lambda x: len(x))\n",
    "    df['mean_len'] = df['copy_text'].apply(lambda x: np.mean([len(w) for w in x]))\n",
    "    df['std_len'] = df['copy_text'].apply(lambda x: np.std([len(w) for w in x]))\n",
    "    if create_embedding:\n",
    "        # create word embedding\n",
    "        df['vector_text'] = df['copy_text'].apply(lambda x: [model[word] for word in x])\n",
    "        # get mean of all words\n",
    "        df['mean_vector_text'] = df['vector_text'].apply(lambda x: sum(x) / len(x) if len(x) != 0 else np.nan)\n",
    "        # create dataframe\n",
    "        features_df = df[['id','mean_vector_text', 'keyword','n_words','n_small_words', 'mean_len',\n",
    "                      'std_len', 'location','n_hashtag','is_question','n_punctuation', 'target', 'split', 'text']]\n",
    "        features_df.loc[pd.isnull(features_df['mean_vector_text']) , 'mean_vector_text'] = np.nan\n",
    "        features_df['mean_vector_text'] = features_df['mean_vector_text'].apply(lambda x:\n",
    "                                                x if type(x) == np.ndarray else np.zeros(300))\n",
    "        f_name = 'v_'\n",
    "        i = 0\n",
    "        for i in tqdm(range(300)):\n",
    "            features_df[f_name + str(i)] = [x[i] for x in features_df['mean_vector_text']]\n",
    "         # preprocessing on 'keyword' column\n",
    "        features_df['main_keyword'] = features_df['keyword'].str.split('%20').apply(lambda x: x[0] if type(x) == list else x)\n",
    "        features_df['additional_keyword'] = features_df['keyword'].apply(lambda x:\n",
    "                                                                re.findall(r'%(\\w+)', x) if type(x) == str else ['Not_given'])\n",
    "        features_df['additional_keyword'] = features_df['additional_keyword'].apply(lambda x: x[0] if len(x) > 0 else 'Not_given')\n",
    "        if replace_nan:\n",
    "            features_df['main_keyword'] = features_df['main_keyword'].fillna('Not given')\n",
    "            features_df['keyword'] = features_df['keyword'].fillna('Not given')\n",
    "            features_df['location'] = features_df['location'].fillna('Not given')\n",
    "        # is main_keyword in text\n",
    "        features_df['is_keywords_in_text'] = features_df[['main_keyword', 'text']].apply(lambda x:\n",
    "                                                        1 if x['main_keyword'] in x['text'].lower() else 0, axis = 1)\n",
    "        return features_df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0fb6c0a43ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# prepr_df = data_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
